[TOC]

# 15 ELK

## 快速入门教程

[ElasticSearch入门](https://www.imooc.com/learn/889)

[.net core大数据之elasticsearch](https://study.163.com/course/courseMain.htm?courseId=1210981803&_trace_c_p_k2_=8a16cd0c27d944bd90f9d919d67a1374)

[ Elasticsearch权威指南]( https://item.jd.com/12788703.html )

[gitub](https://github.com/elastic/elasticsearch/)

https://www.elastic.co/guide/cn/elasticsearch/guide/current/index.html



## 什么是ELK

**ELK**是一个分布式日志分析系统

**E**: ElasticSearch:它是一个是数据库，nosql中的一种，就是实现高亮，分词， 

**L**: logstach:它是一个收集工具，收集各种各样的数据，把数据不仅仅存放到ES 

**K**: kibana:它是一个界面化管理工具，自带好多功能，比如，查询，筛选查询，索引的周期管理 



> 各种nosql:
>
> **reids**:
>
> ​     内存数据库，8大数据结构。它能做筛选查询，统计，聚合查询，所有的数据都放在内存中 
>
> **mongodb**：
>
>    它是一个在redis和mysql之间的一种数据库 ，有些数据在内存，有些数据在硬盘 , 有一家非常牛逼的公司，用的它，存了十亿数据，它能做聚合查询，也能做筛选，而且只 
>
> 有数据格式json就可以存储进去。最大的优点：快速开发，没有具体表结构，随便搞就可以 ，最最牛逼是地图。只有牛逼的公司，尤其是做地理地图， 
>
> **es**：
>
> ​     可以存储各种各样的数据，json 必须可以存，它的数据上百亿 
>
> **hbase**：
>
> ​    大数据生态圈里面比较牛逼，列数据库



## ElasticSearch解决方案

<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1617798365434.png" alt="1617798365434" style="zoom:80%;" />

<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1617798308961.png" alt="1617798308961" style="zoom:80%;" />

![1617798401310](images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1617798401310.png)



## 应用场景

- 站内搜索
- 用ELK组合进行日志的搜集和全文检索
- 业务数据分析



## ES的基本概念

<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1617798523089.png" alt="1617798523089" style="zoom:80%;" />



Elasticsearch有几个核心概念，理解这些概念将极大地简化学习过程。

**1．近实时**
Elasticsearch是一个近实时（Near Real Time，NRT）的数据搜索和分析平台。这意味着从索引文档到可搜索文档都会有一段微小的延迟（通常是1s以内）。



**2．集群**
集群（cluster）是一个或多个节点（node）的集合，这些节点将共同拥有完整的数据，并跨节点提供联合索引、搜索和分析功能。集群由唯一的名称标识（elasticsearch.yml配置文件中对应参数cluster.name），集群的名称是elasticsearch.yml配置文件中最重要的一个配置参数，默认名称为Elasticsearch，节点只能通过集群名称加入集群。
请确保不要在不同的环境中使用相同的集群名称，否则可能会导致节点加入错误的集群。例如，可以使用loggingdev、loggingstage和loggingprod来区分开发、预发布和生产环境的集群。

注意：上面提到了节点实质是一个进程，因此服务器和节点可以是一对多的关系。还有一点需谨记，无论是开发环境、测试环境还是生产环境请配置有意义的节点名称。



**4．索引**
索引（index）是具有某种相似特性的文档集合。例如，可以有存储客户数据的索引，存储产品目录的索引，以及存储订单数据的索引。索引由一个名称（必须全部是小写）标识，当对其中的文档执行索引、搜索、更新和删除操作时，该名称指向这个特定的索引。
在单个集群中，可以定义任意多个索引。



**5．类型**
类型（type）这个概念在7.0版本以后已被彻底移除，因此不再赘述。



**6．文档**

文档（document）是可以被索引的基本信息单元。例如，可以为单个客户创建一个文档，为单个产品创建另一个文档，以及为单个订单创建另一个文档。文档以JSON表示，JSON是一种普遍存在的Internet数据交换格式。在单个索引中，理论上可以存储任意多的文档。



**7．分片和副本**
索引可能会存储大量数据，这些数据可能会超出单个节点的硬件限制。例如，占用1TB磁盘空间的10亿个文档的单个索引可能超出单个节点的磁盘容量，或者速度太慢，无法满足搜索请求的性能要求。
为了解决这个问题，Elasticsearch提供了将索引水平切分为多段（称为分片，shard）的能力。创建索引时，只需定义所需的分片数量。每个分片本身就是一个具有完全功能的独立“索引”，可以分布在集群中的任何节点上。
分片很重要，主要有两个原因：
·　分片可以水平拆分数据，实现大数据存储和分析。
·　可以跨分片（可能在多个节点上）进行分发和并行操作，从而**提高性能和吞吐量**。

如何分配分片以及如何将其文档聚合回搜索请求的机制完全由Elasticsearch管理，并且对用户是透明的。

在随时可能发生故障的网络或云环境中，如果某个分片或节点以某种方式脱机或因何种原因丢失，则**强烈建议用户使用故障转移机制。为此，Elasticsearch提出了将索引分片复制一个或多个拷贝，称为副本（replica）**。
副本很重要，主要有两个原因：
·　副本在分片或节点发生故障时**提供高可用性**。因此，需要注意的是，副本永远不会分配到复制它的原始主分片所在的节点上。也就是分片和对应的副本不可在同一节点上。这很容易理解，如果副本和分片在同一节点上，当机器发生故障时会同时丢失，起不到容错的作用。
·　通过副本机制，可以提**高搜索性能和水平扩展吞吐量**，因为可以在所有副本上并行执行搜索。



总之，每个索引可以分割成多个分片。每个分片可以有零个或多个副本。
可以在创建索引时为每个索引定义分片和副本的数量。创建索引后，还可以**随时动态更改副本的数量**。**分片的数量理论上不可变更**，唯一的办法重建索引，重新定义分片数量。但还是可以使用_shrink和_split API更改索引的分片数量，但这不是通常的做法，预先评估准确的分片数量才是最佳方法。



默认情况下，Elasticsearch中的每个索引都分配一个主分片和一个副本，这意味着如果集群中至少有两个节点，则索引将有一个主分片和另一个副本分片（一个完整副本），每个索引总共有两个分片。

Elasticsearch主要的几个核心概念已经介绍完了。为了让读者加深对这些概念的理解，我们在图1-1中提供了一个真实的Elasticsearch集群。该集群是通过插件head生成的。

<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1617854018642.png" alt="1617854018642" style="zoom:80%;" />



**知识点**：其实每个Elasticsearch分片都是一个完整的Lucene索引。在一个Lucene索引中，可以有大量的文档。从Lucene-5843起，限制为2 147 483 519（=integer.max_value-128）个文档。可以使用_cat/shards api监视分片大小。



注意：分片和副本机制是Elasticsearch实现分布式、水平扩展、高并发、高容错功能的核心。通过分片机制实现大数据的分布式存储，通过副本机制实现了集群的容错、高性能和水平扩展。Elastic是弹性、可伸缩的意思，Elasticsearch的弹性、可伸缩性是建立在分片和副本的基础上的



分片和副本的概念，我们在这里详细讲解这二者的区别和联系：

　本质上分片和副本都是一个完整的Lucenes索引，存储的数据也是完全相同的，都可以称为分片。·

- 假设一个索引定义了3个分片、2个副本，那么总共就是9个分片，其中3个是主分片，每个主分片有2个副本。主分片就是建立索引时首先建立的分片，或者说当主分片失效时会重新选出一个副本作为主分片。　
- 当索引时，数据会首先到达主分片，然后再把请求分发到其他副本。
- 当搜索时，主分片和副本都可以接受请求、搜索和分析数据，二者没有区别。







## REST API

默认情况下，Elasticsearch使用端口9200提供对其REST API的访问，此端口可以配置。

<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1617809489202.png" alt="1617809489202" style="zoom: 80%;" />



## ElasticSearch和Lucene 

Lucene 它是java写的，分词加查询 

ElasticSearch：针对于Lucene封装了一次，使用效果比较好---我们的es集群中一个分片的实例，就 

是lucene实例：是跨语言，支持restful分隔 

<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1617798893526.png" alt="1617798893526" style="zoom:80%;" />



## 环境搭建(Windows)

官网：https://www.elastic.co/cn/downloads/

**切记这些软件包的版本必须一致**



### 安装Java-JDK

略

JDK 10+



### 安装ES

到官网下载ES：https://www.elastic.co/cn/downloads/ 

下载后，解压到如下文件夹：`D:\Program Files (x86)\elk\elasticsearch-7.6.1\`

> 特别注意：
>
> 安装路径中不能包含中文，空格，否则后续会出现很多问题：
>
> 比如，加载分词插件ik 无法启动，抛出异常
>
> 故：下文出现的安装`D:\Program Files (x86)\elk\elasticsearch-7.6.1\`目录 都替换成
>
> `D:\ProgramFiles\elk\elasticsearch-7.6.1\`

> 默认情况下，jvm消耗1G的内存，如果内存不够，到文件夹`elasticsearch-7.6.1\config\jvm.options`
>
> 将
>
> ```txt
> # Xms represents the initial size of total heap space
> # Xmx represents the maximum size of total heap space
> 
> -Xms1g
> -Xmx1g
> 
> ```
>
> 修改为：(最小值为：256M)
>
> ```txt
> -Xms256M
> -Xmx256M
> ```
>
> 



### 开启ES

双击`bin`文件夹下的`elasticsearch.bat`脚本即可，

如何验证？

访问:http://localhost:9200/，返回如下内容：

```json
{
  "name" : "DESKTOP-E6NOF0N",
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "ogZ94d8aQX-jB4avEdohzQ",
  "version" : {
    "number" : "7.6.1",
    "build_flavor" : "default",
    "build_type" : "zip",
    "build_hash" : "aa751e09be0a5072e8570670309b1f12348f023b",
    "build_date" : "2020-02-29T00:15:25.529771Z",
    "build_snapshot" : false,
    "lucene_version" : "8.4.0",
    "minimum_wire_compatibility_version" : "6.8.0",
    "minimum_index_compatibility_version" : "6.0.0-beta1"
  },
  "tagline" : "You Know, for Search"
}
```

表示ES启动成功了。

> 其默认端口为：9200，在`config\elasticsearch.yml`中进行修改：
>
> ```yaml
> # Set the bind address to a specific IP (IPv4 or IPv6):
> #
> #network.host: 192.168.0.1
> #
> # Set a custom port for HTTP:
> #
> #http.port: 9200
> ```



### 插件elasticsearch-head

是一个插件，类似于Kabana

运行head前，先安装 **node.js**

然后把下载的安装包`elasticsearch-head-master.zip`，解压到文件夹：

`D:\Program Files (x86)\elk\elasticsearch-head`下：

打开CMD，执行如下命令

```powershell
# 下载安装包
D:\Program Files (x86)\elk\elasticsearch-head>npm install
# 运行，端口是：9100
D:\Program Files (x86)\elk\elasticsearch-head>npm run start
```

访问:http://localhost:9100/

<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1617803025969.png" alt="1617803025969" style="zoom:80%;" />

没有任何页面数据，这时因为ES没有设置跨域，

head 连接ES，则需要修改ES的`config/elasticsearch.yml` 新增下面两句话解决跨域问题 

```yaml
http.cors.enabled: true 
http.cors.allow-origin: "*" 
```

添加如上配置后，重启ES，

<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1617806628983.png" alt="1617806628983" style="zoom:80%;" />

由于使用的是默认集群名称（elasticsearch），并且Elasticsearch在默认情况下使用单播网络发现算法查找同一台计算机上启动的其他节点，因此可能会意外启动计算机上的多个节点，并使它们都加入单个集群。在这个场景中，可能会在的响应中看到多个节点。



### Kabana

把下载的安装包`kibana-7.6.1-windows-x86_64`，解压到文件夹:

`D:\Program Files (x86)\elk\kibana-7.6.1`

配置文件`config\kibana.yml`

```yaml
1.kibana 可以连接远程的ES，可以在`config\kibana.yml`中进行更改，默认连接是：

#The URLs of the Elasticsearch instances to use for all your queries.
#可以设置连接集群
elasticsearch.hosts: ["http://localhost:9200"]

2.kibana 的访问地址默认是5601，
# Kibana is served by a back end server. This setting specifies the port to use.
#server.port: 5601

3.语言默认是英文的，若要修改为中文
#i18n.locale: "en"
i18n.locale: "zh-CN"
```

设置连接地址`elasticsearch.hosts: ["http://localhost:9200"]`，然后双击`bin\kibana.bat`进行启动

返回访问： http://localhost:5601/

<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1617804781672.png" alt="1617804781672" style="zoom:80%;" />



## 环境搭建(Docker)

**切记这些软件包的版本必须一致**

### 安装ES docker 



自定义网络

```shell
docker network create --driver bridge --subnet 192.168.0.0/16 --gateway 192.168.0.1 mynet 
```



下载es镜像 

```shell
docker pull elasticsearch:7.2.0 
```



运行es 限定内存大小 

```shell
docker run -d --name elasticsearch -p 9200:9200 -p 9300:9300 -e 
"discovery.type=single-node" -e ES_JAVA_OPTS="-Xms100m -Xmx200m" --net mynet 
elasticsearch:7.2.0 
```



### 安装kibana 

下载kibana镜像 

```shell
docker pull kibana:7.2.0 
```

运行 如果地址没起作用，可以在容器里面的usr/share/kibana/config 中修改配置文件 

```shell
docker run -p 5601:5601 -v -d -e ELASTICSEARCH_URL=http://阿里云:9200 --net mynet 

kibana:7.2.0 
```

查看日志 

```sh
docker logs dockerid 
```

验证 

```shell
curl http://localhost:5601 
```



### 安装Logstash 

1、下载Logstash镜像 

```shell
docker pull logstash:7.2.0 
```



2、编辑logstash.yml配置文件 

`logstash.yml`配置文件放在宿主机`/data/elk/logstash`目录下，内容如下： 

```shell
path.config: /usr/share/logstash/conf.d/*.conf 
path.logs: /var/log/logstash 
```



3、编辑test.conf文件 

`test.conf`文件放在宿主机`/data/elk/logstash/conf.d`目录下，内容如下： 

```json
input { 

beats { 

port => 5044 

}

output { 

elasticsearch { hosts => ["云服务器器IP:9200"] } 

stdout { codec => rubydebug } 

}
```



4、启动logstash 

```shell
docker run -d -p 8001:8001 --net mynet --log-driver json-file -v 
/usr/share/logstach/config/logstash.conf:/usr/share/logstash/pipeline/logstash.c 
onf logstash:7.2.0 
```



5、查看容器运行状态 

```shell
docker ps 
docker logs -f xinyar-logstash 
```



6、启动 

```shell
logstash.bat -f logstash.conf
```



在阿里云等云服务上安装，需要防火墙放行的上面使用的端口：

9200:ES

9300:logstash

5601:Kabana





## 基本操作

针对于es里面的任何操作指令，都是大写

打开Kabana的开发工具菜单：http://localhost:5601/app/kibana#/dev_tools/console



查看es所有的索引库

```shell
#查看所有的索引库 
GET _cat/indices 
```

插入数据

```shell
#插入数据 , dbclay-索引，
PUT dbclay/_doc/1 { "name":"clay" }
```

查询数据

```shell
#查询当前库所有数据 
GET dbclay/_doc/_search 
```



### 创建一个索引

现在，创建一个名为customer的索引，然后再次列出所有索引：

```html
PUT /customer?pretty
...

GET /_cat/indices?v
health status index                    uuid                   pri rep docs.count 
yellow open   customer                 NI5nqxgAQMyh36Afj5Xb9A   1   1          0            0       283b           283b
```

第一个命令使用PUT方法创建名为customer的索引。在调用的末尾附加pretty命令，就可以打印友好JSON响应。
第二个命令查询索引索引，?vd显示结果中带行头

customer索引是黄色意味着一些副本尚未分配。此索引发生这种情况的原因是，默认情况下，Elasticsearch为此索引创建了一个副本。因为目前只有一个节点在运行，所以在另一个节点加入集群之前，还不能分配一个副本（为了高可用性）。一旦该副本分配到第二个节点上，该索引的运行状况将变为绿色。



### 索引和查询文档

#### 插入文档

插入一条ID为：1的文档

```shell
PUT /customer/_doc/1?pretty
{
  "name":"John Doe"
}
```



注意：

- type 的概念已从7.0版本开始去掉，所以这里使用`_doc`替换

- 需要注意的是，Elasticsearch并不要求在索引文档之前先显式地创建索引。如果customer索引之前不存在，那么Elasticsearch将自动创建该索引。

  

输出：

```shell
{
  "_index" : "customer",
  "_type" : "_doc",
  "_id" : "1",
  "_version" : 1,
  "result" : "created",
  "_shards" : {
    "total" : 2,
    "successful" : 1,
    "failed" : 0
  },
  "_seq_no" : 0,
  "_primary_term" : 1
}
```



插入文档，自动生成ID

```shell
# 插入文档，自动生成ID
POST /customer/_doc/?pretty
{
  "name":"Hel SS"
}
```





#### 检索文档

检索刚才索引的文档，

```shell
GET /customer/_doc/1?pretty
#输出
{
  "_index" : "customer",
  "_type" : "_doc",
  "_id" : "1",
  "_version" : 1,
  "_seq_no" : 0,
  "_primary_term" : 1,
  "found" : true,
  "_source" : {
    "name" : "John Doe"
  }
}
```

检索所有索引文档

```shell
GET /customer/_doc/_search
```



#### 删除文档

```shell
#删除文档
DELETE /customer/_doc/1?pretty
```



### 删除索引

```shell
DELETE /custmer
{
  "acknowledged" : true
}

```



### 修改数据

#### 覆盖文档

```shell
PUT /customer/_doc/1?pretty
{
  "name":"John Doe"
}

{
  "_index" : "customer",
  "_type" : "_doc",
  "_id" : "1",
  "_version" : 1,
  "result" : "created",
  "_shards" : {
    "total" : 2,
    "successful" : 1,
    "failed" : 0
  },
  "_seq_no" : 0,
  "_primary_term" : 1
}
```

> 刚才已经删除了索引，这个PUT 插入文档的操作 会自动创建索引

现在执行如下操作，

```shell
PUT /customer/_doc/1?pretty
{
  "name":"James"
}
```

这将替换ID为：1的文档



#### 更新文档

Elasticsearch除了能够索引和替换文档外，还可以更新文档。Elasticsearch实际上并不是进行就地更新，每当进行更新时，Elasticsearch会删除旧文档，然后索引一个新文档，但这对用户来说是一次调用。

实际上Elasticsearch的数据存储结构决定了其不能像关系数据库那样进行字段级的更新，所有的更新都是先删除旧文档，再插入一条新文档，但这个过程对用户来说是透明的。

覆盖字段：

```shell
#修改字段
POST /customer/_doc/1?pretty
{
   "name":"Jane Doe"
}
```

追加（覆盖）字段：

```shell
POST /customer/_doc/1?pretty
{
   "name":"Jane Doe",
   "age":20
}
```

如果再执行，（覆盖）

```shell
POST /customer/_doc/1?pretty
{
    "name":"Jane Doe2"
}
```

字段"age":20将会消失



如何实现只更改某个字段，其它字段不会被覆盖：

使用：`POST /customer/_doc/1/_update`会有警告，标准写法是

`POST /customer/_update/1/`

```shell
POST /customer/_doc/1?pretty
{
   "name":"Jane Doe",
    "age":20
}

GET /customer/_doc/1/

POST /customer/_update/1/
{
  "doc":{
      "name":"Jane Doe3"
  }
}

```





### 批量操作

```shell
POST /customer/_bulk
{"index":{"_id": "1"}}
{"name":"Jonh ddd"}
{"index":{"_id": "2"}}
{"name":"Jone EEE"}

## 批量插入两个文档
{
  "took" : 103,
  "errors" : false,
  "items" : [
    {
      "index" : {
        "_index" : "customer",
        "_type" : "_doc",
        "_id" : "1",
        "_version" : 7,
        "result" : "updated",
        "_shards" : {
          "total" : 2,
          "successful" : 1,
          "failed" : 0
        },
        "_seq_no" : 7,
        "_primary_term" : 1,
        "status" : 200
      }
    },
    {
      "index" : {
        "_index" : "customer",
        "_type" : "_doc",
        "_id" : "2",
        "_version" : 1,
        "result" : "created",
        "_shards" : {
          "total" : 2,
          "successful" : 1,
          "failed" : 0
        },
        "_seq_no" : 8,
        "_primary_term" : 1,
        "status" : 201
      }
    }
  ]
}

```





## 模拟数据

 使用bulk API通过cURL命令导入数据文件，如下操作以JSON数据文件为例。 

 使用API导入数据文件时，导入的数据文件大小不能超过50MB。 

```
curl -X PUT "http://{Private network address and port number of the node} /_bulk" -H 'Content-Type: application/json' --data-binary @test.json
```

> 说明：
>
>  其中，-X参数的参数值为命令，如“-X PUT”，-H参数的参数值为消息头，如“-H 'Content-Type: application/json' --data-binary @test.json”。添加的-k参数时，请勿将-k参数放置在参数与参数值之间。 



[官方说明](https://www.elastic.co/guide/en/elasticsearch/reference/current/getting-started-index.html) ，下载测试数据 [`accounts.json`](https://github.com/elastic/elasticsearch/blob/master/docs/src/test/resources/accounts.json) ，其数据基本是这样的：

```json
{ "index": { "_id": "1" } }
{"account_number":1,"balance":39225,"firstname":"Amber","lastname":"Duke","age":32,"gender":"M","address":"880 Holmes Lane","employer":"Pyrami","email":"amberduke@pyrami.com","city":"Brogan","state":"IL"}

{"index":{"_id":"6"}}
{"account_number":6,"balance":5686,"firstname":"Hattie","lastname":"Bond","age":36,"gender":"M","address":"671 Bristol Street","employer":"Netagy","email":"hattiebond@netagy.com","city":"Dante","state":"TN"}

......
```

地址有可能会变，到github的仓储中搜索即可

执行如下命令，导入数据

```shell
# CMD 
cd 到文件accounts.json所在的目录

#导入数据
curl -H "Content-Type: application/json" -XPOST "localhost:9200/bank/_bulk?pretty&refresh" --data-binary "@accounts.json"

#查看索引
curl "localhost:9200/_cat/indices?v=true"

```



结构化数据信息：

```json
"mappings": {
  "_doc": {
    "properties": {
      "account_number": {
        "type": "long"
      },
      "firstname": {
        "type": "text",
        "fields": {
          "keyword": {
            "ignore_above": 256,
            "type": "keyword"
          }
        }
      },
      "address": {
        "type": "text",
        "fields": {
          "keyword": {
            "ignore_above": 256,
            "type": "keyword"
          }
        }
      },
      "balance": {
        "type": "long"
      },
      "gender": {
        "type": "text",
        "fields": {
          "keyword": {
            "ignore_above": 256,
            "type": "keyword"
          }
        }
      },
      "city": {
        "type": "text",
        "fields": {
          "keyword": {
            "ignore_above": 256,
            "type": "keyword"
          }
        }
      },
      "employer": {
        "type": "text",
        "fields": {
          "keyword": {
            "ignore_above": 256,
            "type": "keyword"
          }
        }
      },
      "state": {
        "type": "text",
        "fields": {
          "keyword": {
            "ignore_above": 256,
            "type": "keyword"
          }
        }
      },
      "age": {
        "type": "long"
      },
      "email": {
        "type": "text",
        "fields": {
          "keyword": {
            "ignore_above": 256,
            "type": "keyword"
          }
        }
      },
      "lastname": {
        "type": "text",
        "fields": {
          "keyword": {
            "ignore_above": 256,
            "type": "keyword"
          }
        }
      }
    }
  }
}
```





## 搜索API

执行搜索有两种基本方法：一种是通过REST请求URI发送搜索参数，另一种是通过REST请求主体（body形式）发送搜索参数。请求主体方法表现形式更强大，用更可读的JSON格式定义搜索。



#### REST API 方式

用于搜索的REST API以_search URI结束。下面示例返回bank索引中的所有文档：

```shell
GET /bank/_search?q=*&sort=account_number:asc&pretty
```

先分析一下这个搜索调用。bank参数指明了所用的索引，_search指示这是一个搜索请求（_search endpoint），q=*参数指Elasticsearch匹配指定索引中的所有文档。sort=account_number：asc参数指示使用每个文档的account_number字段按升序对结果进行排序。pretty参数告诉Elasticsearch返回漂亮打印的JSON结果。

响应结果：

```shell
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 1000,
      "relation" : "eq"
    },
    "max_score" : null,
    "hits" : [
      {
        "_index" : "bank",
        "_type" : "_doc",
        "_id" : "0",
        "_score" : null,
        "_source" : {
          "account_number" : 0,
          "balance" : 16623,
          "firstname" : "Bradshaw",
          "lastname" : "Mckenzie",
          "age" : 29,
          "gender" : "F",
          "address" : "244 Columbus Place",
          "employer" : "Euron",
          "email" : "bradshawmckenzie@euron.com",
          "city" : "Hobucken",
          "state" : "CO"
        },
        "sort" : [
          0
        ]
      },
      //.....默认只显示前10个文档
```

关于响应结果，关注以下重点部分：
第2行，took表示Elasticsearch执行搜索所用的时间，单位是ms。
第3行，timed_out用来指示搜索是否超时。
第4行，_shards指示搜索了多少分片，以及搜索成功和失败的分片的计数。
第10行，hits用来实际搜索结果集。
第11行，hits.total是包含与搜索条件匹配的文档总数信息的对象。
第12行，hits.total.value表示总命中计数的值（必须在hits.total.relation上下文中解释）。
第13行，确切来说，默认情况下，hits.total.value是不确切的命中计数，在这种情况下，当hits.total.relation的值是eq时，hits.total.value的值是准确计数。当hits.total.relation的值是gte时，hits.total.value的值是不准确的。

第16行，hits.hits是存储搜索结果的实际数组（默认为前10个文档）。
第35行，hits.sort表示结果排序键（如果请求中没有指定，则默认按分数排序）。

hits.total的准确性由请求参数track_total_hits控制，当设置为true时，请求将准确跟踪总命中数（"relation"："eq"）。它默认为10000，这意味着总命中数精确跟踪多达10000个文档，当结果集大于10000时，hits.total.value的值将是10000，也就是不准确的。可以通过将track_total_hits显式设置为true强制进行精确计数，但这会增大集群资源的开销。



#### 请求主体方式

以下是使用请求body方法进行的相同搜索：

```shell
GET /bank/_search
{
  "query": {"match_all": {}},
  "sort": [
    {
      "account_number":  {
        "order": "asc"
      }
    }
  ]
}
```



## Elasticsearch查询语言

[官方文档](https://www.elastic.co/guide/en/elasticsearch/reference/current/getting-started-search.html#_loading_the_sample_dataset)

**Elasticsearch提供了一种JSON风格的语言，可以使用它来执行查询。这被称为Query DSL**。查询语言功能非常全面，乍一看可能很吓人，但实际学习它的最好方法是从几个基本示例开始。

```shell
GET /customer/_search
{
  "query": {"match_all": {}}
}
```

仔细分析上面的内容，查询（query）部分指明了查询定义是什么，匹配（match）部分是想要运行的查询类型，



### size /from

除了查询参数，还可以传递其他参数来影响搜索结果，这里传入size，

如果未指定size，则默认为10

```shell
GET /bank/_search
{
  "query": {"match_all": {}},
  "size": 20
}
```



from和size参数所起的作用类似于SQL查询中的limit m，n，就是起到分页的作用

如下示例，执行“全部匹配”并返回ID为10到19的文档

```shell
GET /bank/_search
{
  "query": {"match_all": {}},
  "from": 10,
  "size": 10
}
```



### SQL 查询

如果数据结构化的，可以使用SQL查询

```shell
POST /_xpack/sql?format=json
{
   "query":"select * from bank limit 5"
}

{
  "columns" : [
    {
      "name" : "account_number",
      "type" : "long"
    },
    {
      "name" : "address",
      "type" : "text"
    },
    {
      "name" : "age",
      "type" : "long"
    },
    {
      "name" : "balance",
      "type" : "long"
    },
    {
      "name" : "city",
      "type" : "text"
    },
    {
      "name" : "email",
      "type" : "text"
    },
    {
      "name" : "employer",
      "type" : "text"
    },
    {
      "name" : "firstname",
      "type" : "text"
    },
    {
      "name" : "gender",
      "type" : "text"
    },
    {
      "name" : "lastname",
      "type" : "text"
    },
    {
      "name" : "state",
      "type" : "text"
    }
  ],
  "rows" : [
    [
      1,
      "880 Holmes Lane",
      32,
      39225,
      "Brogan",
      "amberduke@pyrami.com",
      "Pyrami",
      "Amber",
      "M",
      "Duke",
      "IL"
    ],
    ....
```

要是在POSTMAN等工具上，请求地址是

```shell
请求：POST http://localhost:9200/_xpack/sql?format=json
参数：
{
   "query":"select * from bank limit 5"
}
```





## 搜索文档

现在开始了解返回的文档字段。默认情况下，搜索会返回完整的JSON文档。这被称为“源”（_source字段）。



### 返回特定字段

如果不希望返回整个源文档，可以只返回源中的几个字段。

```shell
GET /bank/_search
{
  "query": {"match_all": {}},
  "_source":["account_number", "balance"]
}
```

返回结果：

```json
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 1000,
      "relation" : "eq"
    },
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "bank",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : 1.0,
        "_source" : {
          "account_number" : 1,
          "balance" : 39225
        }
      },
```



### 匹配查询

它可以被视为基本的字段化搜索查询（即针对特定字段或一组字段进行的搜索）。
如下示例返回账号（account_number字段）为20的账户信息：

```shell
GET /bank/_search
{
  "query": {"match": {
    "account_number": "20"
  }}
}
```

#### match_all 全匹配

**match_all查询表示搜索指定索引中的所有文档**

```shell
GET /customer/_search
{
  "query": {"match_all": {}}
}
```



#### 字段查询

```shell
GET /bank/_search
{
  "query": { "match": { "address": "mill lane" } }
}
```

查找字段`address` 中包含：`mill`或`lane`的文档



#### match_phrase

查找字段`address` 中包含：`mill lane`这个短语的文档

```shell
GET /bank/_search
{
  "query": { "match_phrase": { "address": "mill lane" } }
}
```



### bool 布尔查询

布尔查询是指使用布尔逻辑的方式把基本的查询组合成复杂的查询。

可以在bool查询中同时组合must、should和must_not子句。此外，还可以在这些bool子句中嵌套bool查询，以模拟任何复杂的多级布尔逻辑。

如下示例返回年龄（age字段）是40岁但不在ID（AHO）中居住（state字段）的所有账户信息

```shell
GET /bank/_search
{
  "query": {
    "bool": {
      "must": [
        { "match": { "age": "40" } }
      ],
      "must_not": [
        { "match": { "state": "ID" } }
      ]
    }
  }
}
```

 customers who are 40 years old, but excludes anyone who lives in Idaho (ID)

must子句指定文档被视为匹配项时必须为真的所有查询, 

must 和 must_not 隐含这AND运算，即两个条件必须同时满足



```shell
GET /bank/_search
{
  "query": {
    "bool": {
      "should": [
        {"match": {"address": "mill"}},
        {"match": {"address": "lane"}}
      ]
    }
  }
}
```

在上面的示例中，should子句指定一个查询列表，其中任何一个查询为真，文档即被视为匹配，也就是只需满足其中一个条件即可。



### filter 条件过滤

For example, the following request uses a range filter to limit the results to accounts with a balance between $20,000 and $30,000 (inclusive).

```shell
GET /bank/_search
{
  "query": {
    "bool": {
      "must": { "match_all": {} },
      "filter": {
        "range": {
          "balance": {
            "gte": 20000,
            "lte": 30000
          }
        }
      }
    }
  }
}
```

 

### 排序

```shell
GET /bank/_search
{
  "query": {"match_all": {}},
  "sort": [
    {
      "state.keyword": { "order": "desc"}
    }
  ]
}
```

必须是`state.keyword": { "order": "desc"}`,

如果是：` "state": { "order": "desc"}`会执行失败

`state`字段的类型：

```json
"state": {
        "type": "text",
        "fields": {
          "keyword": {
            "ignore_above": 256,
            "type": "keyword"
          }
        }
      }
```





### 精确查询

match和term的区别 

>  记得参见[分词规则]()一节，修改分词规则后，与本节示例的不同效果

match，会使用分词器解析！注意：先分词然后再去查询 

term ，直接查询精确的 在定义的时候可以指定查询时候的分词规则，注意只有text类型才是支持的，而且分词类型和搜索分 

词类型必须都要设置 

```shell
DELETE /clayindex

PUT /clayindex
{
  "mappings": {
    "properties": {
      "name": {
        "type": "keyword"
      },
      "address": {
        "type": "text"
      },
      "age": {
        "type": "integer"
      }
    }
  }
}

#插入数据 可以自己生成id /
POST /clayindex/_doc
{
  "name": "曹操",
  "address": "魏国",
  "age": 18
}

POST /clayindex/_doc
{
  "name": "贾诩",
  "address": "魏国",
  "age": 19
}

POST /clayindex/_doc
{
  "name": "诸葛亮",
  "address": "蜀国",
  "age": 37
}

POST /clayindex/_doc
{
  "name": "诸大朗",
  "address": "吴国",
  "age": 38
}

POST /clayindex/_doc
{
  "name": "关羽",
  "address": "蜀国",
  "age": 35
}

POST /clayindex/_doc
{
  "name": "周瑜",
  "address": [
    "吴国",
    "蜀国"
  ],
  "age": 25
}


```

执行如下查询，无匹配数据

```shell
GET /clayindex/_search
{
  "query": {
    "term": {
      "address": "魏国"
    }
  }
}

```

无匹配数据：

```json
{
  "took" : 0,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 0,
      "relation" : "eq"
    },
    "max_score" : null,
    "hits" : [ ]
  }
}

```

term精准查询 查不出数据，因为分词默认分成了 魏和国，

```json
GET _analyze
{
  "analyzer": "keyword",
  "text": "魏国"
}

----输出
{
  "tokens" : [
    {
      "token" : "魏国",
      "start_offset" : 0,
      "end_offset" : 2,
      "type" : "word",
      "position" : 0
    }
  ]
}

```

所以 可以试试查查魏或者国 才会有数据

```shell
GET /clayindex/_search
{
  "query": {
    "term": {
      "address": "魏"
    }
  }
}
```

返回

```json
{
        "_index" : "clayindex",
        "_type" : "_doc",
        "_id" : "hNLgtHgBZUpeNEBJNtWG",
        "_score" : 1.093527,
        "_source" : {
          "name" : "曹操",
          "address" : "魏国",
          "age" : 18
        }
      },
      {
        "_index" : "clayindex",
        "_type" : "_doc",
        "_id" : "hdLitHgBZUpeNEBJjtWs",
        "_score" : 1.093527,
        "_source" : {
          "name" : "贾诩",
          "address" : "魏国",
          "age" : 19
        }
      }
```







### 高亮查询





## 聚合查询

https://www.elastic.co/guide/en/elasticsearch/reference/current/getting-started-aggregations.html

> 知识点：Elasticsearch是一个简单而复杂的产品。之所以说它简单，是因为使用非常简单，用户甚至无须知道其内部的任何实现逻辑，就可直接用于生产环境。在对用户透明和友好的背后，Elasticsearch实现了复杂的分布式机制，强大的索引、查询和分析功能，所以说它又是极其复杂的。



聚合（aggregation）提供了对数据分组和提取统计信息的能力。聚合功能可以理解为大致等同于SQL中的Group By和SQL聚合函数的功能。在Elasticsearch中，可以执行返回命中文档的搜索，同时返回与搜索结果分离的聚合结果。从某种意义上说，这是非常强大和高效的，可以同时运行和查询多个聚合，并一次性获得两个（或多个）操作的结果，避免使用单一的API进行多次网络往返

如下示例州（state.keyword）对所有账户进行分组，然后返回按计数降序排序的前10个（默认值）分组结果

```shell
GET /bank/_search
{
  "size": 0,
  "aggs": {
    "group_by_state": {
      "terms": {
        "field": "state.keyword"}
    }
  }
}
```

类似于SQL：

```sql
 SELECT state,COUNT(*)FROM bank GROUP BY state ORDER BY COUNT(*)DESC LIMIT 10;
```

响应结果：

```json
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 1000,
      "relation" : "eq"
    },
    "max_score" : null,
    "hits" : [ ]
  },
  "aggregations" : {
    "group_by_state" : {
      "doc_count_error_upper_bound" : 0,
      "sum_other_doc_count" : 743,
      "buckets" : [
        {
          "key" : "TX",
          "doc_count" : 30
        },
        {
          "key" : "MD",
          "doc_count" : 28
        },
        {
          "key" : "ID",
          "doc_count" : 27
        },
        {
          "key" : "AL",
          "doc_count" : 25
        },
        {
          "key" : "ME",
          "doc_count" : 25
        },
        {
          "key" : "TN",
          "doc_count" : 25
        },
        {
          "key" : "WY",
          "doc_count" : 25
        },
        {
          "key" : "DC",
          "doc_count" : 24
        },
        {
          "key" : "MA",
          "doc_count" : 24
        },
        {
          "key" : "ND",
          "doc_count" : 24
        }
      ]
    }
  }
}

```

可以看到，TX（德克萨斯）州有30个账户，MD（马里兰）州有28个账户，以此类推。
请注意，size=0设置，表示不显示搜索结果，因为此处只希望在响应结果中看到聚合结果。



基于前面的聚合，再进行聚合，如下示例按州计算账户平均余额（同样，仅返回按计数降序排序的前10个分组结果）

```shell
GET /bank/_search
{
  "size": 0,
  "aggs": {
    "group_by_state": {
      "terms": {
        "field": "state.keyword"},
        "aggs": {
          "average_balance": {
            "avg": {
              "field": "balance"
            }
          }
        }
    }
  }
}

```

请仔细分析上面的示例，如何将平均余额聚合嵌套在按州聚合之内。**这是所有聚合的通用模式。可以在聚合中任意嵌套聚合**，以从数据中提取所需的透视统计信息。

返回结果：

```shell
{
  "took" : 9,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 1000,
      "relation" : "eq"
    },
    "max_score" : null,
    "hits" : [ ]
  },
  "aggregations" : {
    "group_by_state" : {
      "doc_count_error_upper_bound" : 0,
      "sum_other_doc_count" : 743,
      "buckets" : [
        {
          "key" : "TX",
          "doc_count" : 30,
          "average_balance" : {
            "value" : 26073.3
          }
        },
        {
          "key" : "MD",
          "doc_count" : 28,
          "average_balance" : {
            "value" : 26161.535714285714
          }
        },
        {
          "key" : "ID",
          "doc_count" : 27,
          "average_balance" : {
            "value" : 24368.777777777777
          }
        },
        {
          "key" : "AL",
          "doc_count" : 25,
          "average_balance" : {
            "value" : 25739.56
          }
        },
        {
          "key" : "ME",
          "doc_count" : 25,
          "average_balance" : {
            "value" : 21663.0
          }
        },
        {
          "key" : "TN",
          "doc_count" : 25,
          "average_balance" : {
            "value" : 28365.4
          }
        },
        {
          "key" : "WY",
          "doc_count" : 25,
          "average_balance" : {
            "value" : 21731.52
          }
        },
        {
          "key" : "DC",
          "doc_count" : 24,
          "average_balance" : {
            "value" : 23180.583333333332
          }
        },
        {
          "key" : "MA",
          "doc_count" : 24,
          "average_balance" : {
            "value" : 29600.333333333332
          }
        },
        {
          "key" : "ND",
          "doc_count" : 24,
          "average_balance" : {
            "value" : 26577.333333333332
          }
        }
      ]
    }
  }
}

```



如下示例中，按降序对账户平均余额进行排序

```shell
GET /bank/_search
{
  "size": 0,
  "aggs": {
    "group_by_state": {
      "terms": {
        "field": "state.keyword",
        "order": {
          "average_balance": "desc"  //用到了下面定义的聚合名称average_balance
        }
      },
      "aggs": {
        "average_balance": {
          "avg": {
            "field": "balance"
          }
        }
      }
    }
  }
}
```

返回结果(部分)：

```json
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 1000,
      "relation" : "eq"
    },
    "max_score" : null,
    "hits" : [ ]
  },
  "aggregations" : {
    "group_by_state" : {
      "doc_count_error_upper_bound" : -1,
      "sum_other_doc_count" : 827,
      "buckets" : [
        {
          "key" : "CO",
          "doc_count" : 14,
          "average_balance" : {
            "value" : 32460.35714285714
          }
        },
        {
          "key" : "NE",
          "doc_count" : 16,
          "average_balance" : {
            "value" : 32041.5625
          }
        },
        {
          "key" : "AZ",
          "doc_count" : 14,
          "average_balance" : {
            "value" : 31634.785714285714
          }
        },
        {
          "key" : "MT",
          "doc_count" : 17,
          "average_balance" : {
            "value" : 31147.41176470588
          }
        },
        {
          "key" : "VA",
          "doc_count" : 16,
          "average_balance" : {
            "value" : 30600.0625
          }
        },
        {
          "key" : "GA",
          "doc_count" : 19,
          "average_balance" : {
            "value" : 30089.0
          }
        },
        {
          "key" : "MA",
          "doc_count" : 24,
          "average_balance" : {
            "value" : 29600.333333333332
          }
        },
        {
          "key" : "IL",
          "doc_count" : 22,
          "average_balance" : {
            "value" : 29489.727272727272
          }
        },
        {
          "key" : "NM",
          "doc_count" : 14,
          "average_balance" : {
            "value" : 28792.64285714286
          }
        },
        {
          "key" : "LA",
          "doc_count" : 17,
          "average_balance" : {
            "value" : 28791.823529411766
          }
        }
      ]
    }
  }
}

```



如下示例，演示了如何按年龄段（20～29岁、30～39岁和40～49岁）进行分组，然后再按性别（gender

keyword）分组，最后得到每个年龄段每个性别的账户平均余额：

```shell
GET /bank/_search
{
  "size": 0,
  "aggs": {
    "group_by_age": {   //按年龄段分组
      "range": {
        "field": "age",
        "ranges": [
          {
            "from": 20,
            "to": 30
          },
          {
            "from": 30,
            "to": 40
          },
          {
            "from": 40,
            "to": 50
          }
        ]
      },
      "aggs": {
        "group_by_state": {  //在年龄段分组的基础上，再按性别分组
          "terms": {
            "field": "gender.keyword",
            "order": {
              "average_balance": "desc"
            }
          },
          "aggs": {         //年龄段+性别分组，求balance的平均值
            "average_balance": {
              "avg": {
                "field": "balance"
              }
            }
          }
        }
      }
    }
  }
}
```

返回结果：

```json
{
  "took" : 5, //ms
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 1000,
      "relation" : "eq"
    },
    "max_score" : null,
    "hits" : [ ]
  },
  "aggregations" : {
    "group_by_age" : {
      "buckets" : [
        {
          "key" : "20.0-30.0",
          "from" : 20.0,
          "to" : 30.0,
          "doc_count" : 451,
          "group_by_state" : {
            "doc_count_error_upper_bound" : 0,
            "sum_other_doc_count" : 0,
            "buckets" : [
              {
                "key" : "M",
                "doc_count" : 232,
                "average_balance" : {
                  "value" : 27374.05172413793
                }
              },
              {
                "key" : "F",
                "doc_count" : 219,
                "average_balance" : {
                  "value" : 25341.260273972603
                }
              }
            ]
          }
        },
        {
          "key" : "30.0-40.0",
          "from" : 30.0,
          "to" : 40.0,
          "doc_count" : 504,
          "group_by_state" : {
            "doc_count_error_upper_bound" : 0,
            "sum_other_doc_count" : 0,
            "buckets" : [
              {
                "key" : "F",
                "doc_count" : 253,
                "average_balance" : {
                  "value" : 25670.869565217392
                }
              },
              {
                "key" : "M",
                "doc_count" : 251,
                "average_balance" : {
                  "value" : 24288.239043824702
                }
              }
            ]
          }
        },
        {
          "key" : "40.0-50.0",
          "from" : 40.0,
          "to" : 50.0,
          "doc_count" : 45,
          "group_by_state" : {
            "doc_count_error_upper_bound" : 0,
            "sum_other_doc_count" : 0,
            "buckets" : [
              {
                "key" : "F",
                "doc_count" : 21,
                "average_balance" : {
                  "value" : 27992.571428571428
                }
              },
              {
                "key" : "M",
                "doc_count" : 24,
                "average_balance" : {
                  "value" : 26474.958333333332
                }
              }
            ]
          }
        }
      ]
    }
  }
}

```





## 分词

```shell
GET _analyze
{
  "text": "我不喜欢你"
}
```

输出结果：

```json
{
  "tokens" : [
    {
      "token" : "我",
      "start_offset" : 0,
      "end_offset" : 1,
      "type" : "<IDEOGRAPHIC>",
      "position" : 0
    },
    {
      "token" : "不",
      "start_offset" : 1,
      "end_offset" : 2,
      "type" : "<IDEOGRAPHIC>",
      "position" : 1
    },
    {
      "token" : "喜",
      "start_offset" : 2,
      "end_offset" : 3,
      "type" : "<IDEOGRAPHIC>",
      "position" : 2
    },
    {
      "token" : "欢",
      "start_offset" : 3,
      "end_offset" : 4,
      "type" : "<IDEOGRAPHIC>",
      "position" : 3
    },
    {
      "token" : "你",
      "start_offset" : 4,
      "end_offset" : 5,
      "type" : "<IDEOGRAPHIC>",
      "position" : 4
    }
  ]
}

```



### IK分词插件

默认分词器，分出的词语为[我，不，喜，欢，你]，这不符合中文的分词要求，如果项目中存在大量的中文，需要自己安装中文分词插件，这里我们使用`elasticsearch-analysis-ik-7.6.1`。

安装步骤：

- 下载

​     对应的版本：https://github.com/medcl/elasticsearch-analysis-ik:`elasticsearch-analysis-ik-7.6.1`



- 拷贝到插件目录

​        在ES安装目录`D:\Program Files (x86)\elk\elasticsearch-7.6.1\plugins`下新建文件夹`ik`（不能是其它名字），把压缩包内的文件拷贝到该目录下。

​      插件启动失败，安装安装目录存在空格

将安装目录修改为：

```shell
D:\ProgramFiles\elk\elasticsearch-7.6.1\plugins`
```



- 重启ES

  Kabana也得先关闭，否则重新启动ES失败



使用ik分词器

```shell
GET _analyze
{
   "analyzer": "ik_max_word",
  "text": "我不喜欢你"
}
```

输出：

```shell
{
  "tokens" : [
    {
      "token" : "我",
      "start_offset" : 0,
      "end_offset" : 1,
      "type" : "CN_CHAR",
      "position" : 0
    },
    {
      "token" : "不喜欢",
      "start_offset" : 1,
      "end_offset" : 4,
      "type" : "CN_WORD",
      "position" : 1
    },
    {
      "token" : "喜欢",
      "start_offset" : 2,
      "end_offset" : 4,
      "type" : "CN_WORD",
      "position" : 2
    },
    {
      "token" : "你",
      "start_offset" : 4,
      "end_offset" : 5,
      "type" : "CN_CHAR",
      "position" : 3
    }
  ]
}

```



#### 扩展自己的字典

上面的分词如如下分词：[我, 不喜欢，喜欢，你]，但是没有：[喜欢你]，这是需要添加自己的字典。

步骤：

- 在新建字典文件`ik\confi\my_extr.dic`

  ```text
  喜欢你
  ```

  

- 把这个字典文件添加到``ik\confi\IKAnalyzer.cfg.xml`文件中

  ```xml
  <?xml version="1.0" encoding="UTF-8"?>
  <!DOCTYPE properties SYSTEM "http://java.sun.com/dtd/properties.dtd">
  <properties>
  	<comment>IK Analyzer 扩展配置</comment>
  	<!--用户可以在这里配置自己的扩展字典 -->
  	<entry key="ext_dict"></entry>
  	 <!--用户可以在这里配置自己的扩展停止词字典-->
  	<entry key="ext_stopwords"></entry>
  	<!--用户可以在这里配置远程扩展字典 -->
  	<!-- <entry key="remote_ext_dict">words_location</entry> -->
  	<!--用户可以在这里配置远程扩展停止词字典-->
  	<!-- <entry key="remote_ext_stopwords">words_location</entry> -->
  </properties>
  ```

  修改为：

  ```xml
  <?xml version="1.0" encoding="UTF-8"?>
  <!DOCTYPE properties SYSTEM "http://java.sun.com/dtd/properties.dtd">
  <properties>
     ......
  	<entry key="ext_dict">my_extr.dic</entry>
     .....
  </properties>
  
  ```

- 重启ES

  Kabana也得先关闭，否则重新启动ES失败

使用ik分词器

```shell
GET _analyze
{
   "analyzer": "ik_max_word",
  "text": "我不喜欢你"
}
```

输出：

```json
{
  "tokens" : [
    {
      "token" : "我",
      "start_offset" : 0,
      "end_offset" : 1,
      "type" : "CN_CHAR",
      "position" : 0
    },
    {
      "token" : "不喜欢",
      "start_offset" : 1,
      "end_offset" : 4,
      "type" : "CN_WORD",
      "position" : 1
    },
    {
      "token" : "喜欢你",
      "start_offset" : 2,
      "end_offset" : 5,
      "type" : "CN_WORD",
      "position" : 2
    },
    {
      "token" : "喜欢",
      "start_offset" : 2,
      "end_offset" : 4,
      "type" : "CN_WORD",
      "position" : 3
    },
    {
      "token" : "你",
      "start_offset" : 4,
      "end_offset" : 5,
      "type" : "CN_CHAR",
      "position" : 4
    }
  ]
}

```

看到结果中，都出了一个分词：[喜欢你]



**通过自定义的字典，可以添加自己的分词字典，用于特殊行业的专业名称的分词，比如：药名等**



### 分词规则

**在定义索引的时候可以指定查询时候的分词规则**

注意只有text类型才是支持的，而且分词类型和搜索分,词类型必须都要设置 

```shell
PUT /clayindex3 
{
  "mappings": {
    "properties": {
      "name": {
        "type": "keyword"
      },
      "address": {
        "type": "text",
        "analyzer": "standard",
        "search_analyzer": "standard"
      },
      "age": {
        "type": "integer"
      }
    }
  }
}
```



**分词和搜索分词的类型可以不同** 

```shell
PUT /clayindex4 
{
  "mappings": {
    "properties": {
      "name": {
        "type": "keyword"
      },
      "address": {
        "type": "text",
        "analyzer": "ik_max_word",
        "search_analyzer": "standard"
      },
      "age": {
        "type": "integer"
      }
    }
  }
}
```



>  注意，针对 text 类型区分精确查询和模糊查询，如果是keyword类型的话不区分，因为没有给 
>
> keyword类型做分词 



示例：

- 插入模拟数据

  ```shell
  #插入数据 可以自己生成id /
  POST /clayindex4/_doc
  {
    "name": "曹操",
    "address": "魏国",
    "age": 18
  }
  
  POST /clayindex4/_doc
  {
    "name": "贾诩",
    "address": "魏国",
    "age": 19
  }
  
  POST /clayindex4/_doc
  {
    "name": "诸葛亮",
    "address": "蜀国",
    "age": 37
  }
  
  POST /clayindex4/_doc
  {
    "name": "诸大朗",
    "address": "吴国",
    "age": 38
  }
  
  POST /clayindex4/_doc
  {
    "name": "关羽",
    "address": "蜀国",
    "age": 35
  }
  
  POST /clayindex4/_doc
  {
    "name": "周瑜",
    "address": [
      "吴国",
      "蜀国"
    ],
    "age": 25
  }
  
  GET /clayindex4/_search
  
  ```

  

- 精确查询

  精确查询返回了`address=魏国`的全部数据

  ```shell
  GET /clayindex4/_search
  {
    "query": {
      "term": {
        "address": "魏国"
      }
    }
  }
  ```

  返回

  ```json
  {
          "_index" : "clayindex4",
          "_type" : "_doc",
          "_id" : "itIztXgBZUpeNEBJF9Wf",
          "_score" : 1.093527,
          "_source" : {
            "name" : "曹操",
            "address" : "魏国",
            "age" : 18
          }
        },
        {
          "_index" : "clayindex4",
          "_type" : "_doc",
          "_id" : "i9IztXgBZUpeNEBJGNUJ",
          "_score" : 1.093527,
          "_source" : {
            "name" : "贾诩",
            "address" : "魏国",
            "age" : 19
          }
        }
  ```

  

  这时因为我们在定义索引的时候，字段`address`使用了分词`ik_max_word`

  ```shell
  "address": {
          "type": "text",
          "analyzer": "ik_max_word",
          "search_analyzer": "standard"
        }
  ```

  这时，精确查询使用的是分词`魏国`，而不是`魏`或者`国`

  ```shell
  GET _analyze
  {
    "analyzer": "keyword",
    "text": "魏国"
  }
  
  ----输出
  {
    "tokens" : [
      {
        "token" : "魏国",
        "start_offset" : 0,
        "end_offset" : 2,
        "type" : "word",
        "position" : 0
      }
    ]
  }
  ```

  执行如下查询，反而无数据

  ```shell
  GET /clayindex4/_search
  {
    "query": {
      "term": {
        "address": "魏"
      }
    }
  }
  ```

  返回

  ```json
  {
    "took" : 0,
    "timed_out" : false,
    "_shards" : {
      "total" : 1,
      "successful" : 1,
      "skipped" : 0,
      "failed" : 0
    },
    "hits" : {
      "total" : {
        "value" : 0,
        "relation" : "eq"
      },
      "max_score" : null,
      "hits" : [ ]
    }
  }
  
  ```

  

  



## 字段类型

字符串：text，keyword 

数值: long，integer,short,byte,double,float,half float,scaled float 

日期类型：date 

布尔类型：boolean 

二进制类型:binary

其中，

keyword：存储数据时候，不会分词建立索引 

text：存储数据时候，会自动分词，并生成索引（这是很智能的，但在有些字段里面是没用的，所以对 

于有些字段使用text则浪费了空间）。 



## 创建索引约束类型

```shell
# 默认是_doc 完整版:PUT /Test2/_doc 默认的就是_doc 

PUT /test2
{
  "mappings": {
    "properties": {
      "name": {
        "type": "text"
      },
      "age": {
        "type": "long"
      },
      "actiontime": {
        "type": "date",
        "format": "yyyy-MM-dd HH:mm:ss"
      },
      "describe": {
        "type": "keyword"
      }
    }
  }
}

GET /test2

PUT /test2/_doc/1
{
  "name": "name2",
  "age": 11,
  "actiontime": "2018-09-24 19:23:45",
  "describe": 12
}

GET /test2/_search/
```



### text 和 keyword 类型的区别

> 记得参见[分词规则]()一节，修改分词规则后，与本节示例的不同效果

ElasticSearch 5.0以后，string字段被拆分成两种新的数据类型: 

**Text**：

会分词，然后根据分词后的内容建立倒排索引（反向索引），不支持聚合 



**keyword**：

不进行分词，直接直接根据字符串内容建立倒排索引（反向索引） 支持聚合 。这种类型一般用于

名字，专业名称，药品 ，商品名称等



示例：

创建索引`clayindex`，并插入测试数据

```shell
DELETE /clayindex

PUT /clayindex
{
  "mappings": {
    "properties": {
      "name": {
        "type": "keyword"
      },
      "address": {
        "type": "text"
      },
      "age": {
        "type": "integer"
      }
    }
  }
}

#插入数据 可以自己生成id /
POST /clayindex/_doc
{
  "name": "曹操",
  "address": "魏国",
  "age": 18
}

POST /clayindex/_doc
{
  "name": "贾诩",
  "address": "魏国",
  "age": 19
}

POST /clayindex/_doc
{
  "name": "诸葛亮",
  "address": "蜀国",
  "age": 37
}

POST /clayindex/_doc
{
  "name": "关羽",
  "address": "蜀国",
  "age": 35
}

POST /clayindex/_doc
{
  "name": "诸大朗",
  "address": "吴国",
  "age": 38
}

POST /clayindex/_doc
{
  "name": "周瑜",
  "address": [
    "吴国",
    "蜀国"
  ],
  "age": 25
}
```

运行如下查询：

```shell
GET /clayindex/_search?q=address:魏国
或者
GET /clayindex/_search 
{
  "query": {"match": {
    "address": "魏国"
  }}
}
```

返回结果，返回所有数据，这是为什么？

因为：`address`字段是`text`类型，所以被分词 了：[魏，国]



查询

```shell
GET /clayindex/_search?q=name:诸葛亮
或
GET /clayindex/_search 
{
  "query": {"match": {
    "name": "诸葛亮"
  }}
}
```

只返回：

```json
      {
        "_index" : "clayindex",
        "_type" : "_doc",
        "_id" : "htLitHgBZUpeNEBJldW6",
        "_score" : 1.540445,
        "_source" : {
          "name" : "诸葛亮",
          "address" : "蜀国",
          "age" : 37
        }
      }
```

查询

```shell
GET /clayindex/_search?q=name:诸

GET /clayindex/_search 
{
  "query": {"match": {
    "name": "诸"
  }}
}
```

无匹配数据



## term,match,分词、搜索分词组合

### 使用默认分词

```shell
PUT /clayindex
{
  "mappings": {
    "properties": {
      "name": {
        "type": "keyword"
      },
      "address": {
        "type": "text"
      },
      "age": {
        "type": "integer"
      }
    }
  }
}
```

插入数据后，`address`字段的分词：[魏，国]



执行查询：

```shell
-----------------------------match
GET /clayindex/_search
{
  "query": {
    "match": {
      "address": "魏国"
    }
  }
}

//能返回"address" : "蜀国"的文档 

-----------------------------match
GET /clayindex/_search
{
  "query": {
    "match": {
      "address": "魏"
    }
  }
}

//返回"address" : "魏国"的文档 

-----------------------------match_phrase
GET /clayindex/_search
{
  "query": {
    "match_phrase": {
      "address": "魏国" //或者"address": "魏"
    }
  }
}

//返回"address" : "魏国"的文档 

-----------------------------term

GET /clayindex/_search
{
  "query": {
    "term": {
      "address": "魏"
    }
  }
}
//返回"address" : "魏国"的文档 

-----------------------------term
GET /clayindex/_search
{
  "query": {
    "term": {
      "address": "魏国"
    }
  }
}
//无匹配数据
```



### 使用分词规则

- 创建索引时，使用分词规则`"analyzer": "ik_max_word"`, 但是搜索分词是使用标准的：

```shell
PUT /clayindex4 
{
  "mappings": {
    "properties": {
      "name": {
        "type": "keyword"
      },
      "address": {
        "type": "text",
        "analyzer": "ik_max_word",
        "search_analyzer": "standard"
      },
      "age": {
        "type": "integer"
      }
    }
  }
}
```

插入数据后，`address`字段的分词：[魏国]

```shell
#插入数据 可以自己生成id /
POST /clayindex4/_doc
{
  "name": "曹操",
  "address": "魏国",
  "age": 18
}

POST /clayindex4/_doc
{
  "name": "贾诩",
  "address": "魏国",
  "age": 19
}

POST /clayindex4/_doc
{
  "name": "贾诩2",
  "address": "魏国2",
  "age": 19
}

POST /clayindex4/_doc
{
  "name": "诸葛亮",
  "address": "蜀国",
  "age": 37
}

POST /clayindex4/_doc
{
  "name": "诸大朗",
  "address": "吴国",
  "age": 38
}

POST /clayindex4/_doc
{
  "name": "关羽",
  "address": "蜀国",
  "age": 35
}

POST /clayindex4/_doc
{
  "name": "周瑜",
  "address": [
    "吴国",
    "蜀国"
  ],
  "age": 25
}

GET /clayindex4/_search
```



执行查询：

```shell
GET /clayindex4/_search
-----------------------------match
GET /clayindex4/_search
{
  "query": {
    "match": {
      "address": "魏国"
    }
  }
}

//无匹配数据档 

-----------------------------match
GET /clayindex4/_search
{
  "query": {
    "match": {
      "address": "魏"
    }
  }
}

//无匹配数据档 

-----------------------------match_phrase
GET /clayindex4/_search
{
  "query": {
    "match_phrase": {
      "address": "魏国"
    }
  }
}
//无匹配数据档 

-----------------------------term

GET /clayindex4/_search
{
  "query": {
    "term": {
      "address": "魏"
    }
  }
}
//无匹配数据

-----------------------------term
GET /clayindex4/_search
{
  "query": {
    "term": {
      "address": "魏国"
    }
  }
}

//返回"address" : "魏国"的文档 
```



- 创建索引时，使用分词规则`"analyzer": "ik_max_word"`, 但是搜索分词是使用`ik_max_word`：

```shell
PUT /clayindex5 
{
  "mappings": {
    "properties": {
      "name": {
        "type": "keyword"
      },
      "address": {
        "type": "text",
        "analyzer": "ik_max_word",
        "search_analyzer": "ik_max_word"
      },
      "age": {
        "type": "integer"
      }
    }
  }
}
```

插入模拟数据：

```shell
#插入数据 可以自己生成id /
POST /clayindex5/_doc
{
  "name": "曹操",
  "address": "魏国",
  "age": 18
}

POST /clayindex5/_doc
{
  "name": "贾诩",
  "address": "魏国",
  "age": 19
}

POST /clayindex5/_doc
{
  "name": "诸葛亮",
  "address": "蜀国",
  "age": 37
}

POST /clayindex5/_doc
{
  "name": "诸大朗",
  "address": "吴国",
  "age": 38
}

POST /clayindex5/_doc
{
  "name": "关羽",
  "address": "蜀国",
  "age": 35
}

POST /clayindex5/_doc
{
  "name": "周瑜",
  "address": [
    "吴国",
    "蜀国"
  ],
  "age": 25
}

GET /clayindex5/_search
```

执行如下查询，

```shell
-----------------------------match
GET /clayindex5/_search
{
  "query": {
    "match": {
      "address": "魏国"
    }
  }
}

能返回"address" : "蜀国"的文档 

-----------------------------match
GET /clayindex5/_search
{
  "query": {
    "match": {
      "address": "魏"
    }
  }
}

//返回"address" : "魏国"的文档 

-----------------------------match_phrase
GET /clayindex5/_search
{
  "query": {
    "match_phrase": {
      "address": "魏国"
    }
  }
}
//返回"address" : "魏国"的文档  

-----------------------------term

GET /clayindex5/_search
{
  "query": {
    "term": {
      "address": "魏"
    }
  }
}
//返回"address" : "魏国"的文档 

-----------------------------term
GET /clayindex5/_search
{
  "query": {
    "term": {
      "address": "魏国"
    }
  }
}

//无匹配数据
```



- 创建索引时，使用分词规则`"analyzer": "ik_max_word"`, 但是搜索分词是使用`ik_smart`：

  https://github.com/medcl/elasticsearch-analysis-ik

```she
PUT /clayindex5 
{
  "mappings": {
    "properties": {
      "name": {
        "type": "keyword"
      },
      "address": {
        "type": "text",
        "analyzer": "ik_max_word",
        "search_analyzer": "ik_smart"
      },
      "age": {
        "type": "integer"
      }
    }
  }
}
```

插入模拟数据：

```shell
#插入数据 可以自己生成id /
POST /clayindex6/_doc
{
  "name": "曹操",
  "address": "魏国",
  "age": 18
}

POST /clayindex6/_doc
{
  "name": "贾诩",
  "address": "魏国",
  "age": 19
}

POST /clayindex6/_doc
{
  "name": "诸葛亮",
  "address": "蜀国",
  "age": 37
}

POST /clayindex6/_doc
{
  "name": "诸大朗",
  "address": "吴国",
  "age": 38
}

POST /clayindex6/_doc
{
  "name": "关羽",
  "address": "蜀国",
  "age": 35
}

POST /clayindex6/_doc
{
  "name": "周瑜",
  "address": [
    "吴国",
    "蜀国"
  ],
  "age": 25
}

GET /clayindex6/_search
```

执行如下查询，

```shell
-----------------------------match
GET /clayindex6/_search
{
  "query": {
    "match": {
      "address": "魏国"
    }
  }
}

//返回"address" : "魏国"的文档 

-----------------------------match
GET /clayindex6/_search
{
  "query": {
    "match": {
      "address": "魏"
    }
  }
}

//无匹配数据

-----------------------------match_phrase
GET /clayindex6/_search
{
  "query": {
    "match_phrase": {
      "address": "魏国"
    }
  }
}
//返回"address" : "魏国"的文档  

-----------------------------term

GET /clayindex6/_search
{
  "query": {
    "term": {
      "address": "魏"
    }
  }
}
//无匹配数据

-----------------------------term
GET /clayindex6/_search
{
  "query": {
    "term": {
      "address": "魏国"
    }
  }
}

//返回"address" : "魏国"的文档 
```

**使用分词规则`"analyzer": "ik_max_word"`, 但是搜索分词是使用`ik_smart`才真正是我们想要的预期结果**



## ES原理剖析



### 倒排索引

<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1617955622594.png" alt="1617955622594" style="zoom:80%;" />



**倒排索引**

<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1617955655929.png" alt="1617955655929" style="zoom:80%;" />

分词，然后每个分词作为主键



案例分析：

<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1617955797572.png" alt="1617955797572" style="zoom:80%;" />

<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1617955809970.png" alt="1617955809970" style="zoom:80%;" />

倒排索引的创建过程就是建立Term到文档ID的映射关系。



搜索引擎可分为三个过程：分析、索引和搜索。
分析是通过一定的分析算法把字段的值切分为**Term（字段文本分词得到的词条）**的过程。索引（indexing）是将数据源通过一定方式提取信息，建立倒排索引的过程。搜索（search）是根据用户查询请求，搜索创建的索引，返回索引内容的过程。



### 索引压缩法

Term放在内存中，很消耗内存，需对其进行压缩，压缩算法是：**索引压缩法**(FST), 对关键字进行压缩 

<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1617956743699.png" alt="1617956743699" style="zoom:80%;" />

如上图，把原来的19个字母压缩成9个字母

FST: 索引压缩法

在内存中共享key的前缀(a)和后缀(z)， 在内存中很少的内存，表达很大的信息

<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1617957207248.png" alt="1617957207248" style="zoom:80%;" />

上图说明，把24 bytes的数据变成了7bytes的数据

73:73

300 ：73+**227**

320：73+227+**2**

332：73+227+2+**30**

......



73 227 2  |  30 11 29 

分组处理



8:73 227 2 --> 最大值227转化为二进制占8个bit ，所以8是该组每个数字分配的最大位数



### 跳跃表

<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1617969102057.png" alt="1617969102057" style="zoom:80%;" />

<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1617969136029.png" alt="1617969136029" style="zoom:80%;" />

通过跳跃表的方式实现快速查找



### 总结

ElasticSearch索引的思路:
它是通过各种各样的变态到极致的算法还有数据结构，实现让内存中的数据少量，然后表示更多的数据（快大），在通过跳跃表的方式实现快速的根据我们联合查询的多个集合，找到交集，并集，还有差集



## 分布式原理

<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1617974075162.png" alt="1617974075162" style="zoom:80%;" />

Elasticseasrch的架构遵循其基本特性：高扩展性和高可用性。
·　高扩展性：向Elasticsearch添加节点非常简单，新节点配置完成启动后，会自动加入集群，同时数据会重新均衡。
·　高可用性：Elasticsearch是分布式的，每个节点都会有备份，所以一两个节点宕机并不会造成服务中断，集群会通过备份进行自动恢复。



Elasticsearch的分布式原理是基于如下4个方面实现的：
·　分片机制（Shard）。将完整数据切割成N份存储在不同的节点上，解决单个节点资源的限制。
·　副本机制（Replica）。在设置了副本后，集群中某个节点宕机后，通过副本可以快速对缺失数据进行恢复。
·　集群发现机制（Discovery）。当启动了一个Elasticsearch节点（其实是一个Java进程）时，会自动发现集群并加入。
·　负载均衡（Relocate）。Elasticsearch会对所有的分片进行均衡地分配。当新增或减少节点时，不用任何干预，Elasticsearch会自动进行数据的重新分配，以达到均衡。



## ES集群

把下载的ES拷贝三份，如下所示：

<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1618295648689.png" alt="1618295648689" style="zoom: 80%;" />



### 配置文件

用如下配置文件分别配置：

主节点：

```ya
# ================= Elasticsearch Configuration ===================
# 配置es的集群名称, es会自动发现在同一网段下的es,如果在同一网段下有多个集群,就可以用这个属性来区分不同的集群｡
cluster.name: elasticsearch
# 节点名称
node.name: node-001
# 指定该节点是否有资格被选举成为node
node.master: true
cluster.initial_master_nodes: ["127.0.0.1:9300"]

# 指定该节点是否存储索引数据,默认为true｡
node.data: true
# 设置绑定的ip地址还有其它节点和该节点交互的ip地址,本机ip
network.host: 127.0.0.1
# 指定http端口,你使用head､kopf等相关插件使用的端口
http.port: 9200
# 设置节点间交互的tcp端口,默认是9300｡
transport.tcp.port: 9300
#设置集群中master节点的初始列表,可以通过这些节点来自动发现新加入集群的节点｡
#因为下两台elasticsearch的port端口会设置成9301 和 9302 所以写入两台#elasticsearch地址的完整路径
discovery.zen.ping.unicast.hosts: ["127.0.0.1:9300","127.0.0.1:9301","127.0.0.1:9302"]
#如果要使用head,那么需要解决跨域问题,使head插件可以访问es
http.cors.enabled: true
http.cors.allow-origin: "*"
```

两个从节点分别是：

从节点1：

```yaml
# ================= Elasticsearch Configuration =================== 
# 配置es的集群名称, es会自动发现在同一网段下的es,如果在同一网段下有多个集群,就可以用这个属性来区分不同的集群｡ 
cluster.name: elasticsearch 
# 节点名称 
node.name: node-002 
# 指定该节点是否有资格被选举成为node 
node.master: true 
# 指定该节点是否存储索引数据,默认为true｡ 
node.data: true
# 设置绑定的ip地址还有其它节点和该节点交互的ip地址,本机ip
network.host: 127.0.0.1
# 指定http端口,你使用head､kopf等相关插件使用的端口 
cluster.initial_master_nodes: ["127.0.0.1:9300"]
http.port: 9201 
# 设置节点间交互的tcp端口,默认是9300｡ 
transport.tcp.port: 9301 
#设置集群中master节点的初始列表,可以通过这些节点来自动发现新加入集群的节点｡
#因为下一台elasticsearch的port端口会设置成9301  所以写入两台#elasticsearch地址的完整路径 
discovery.zen.ping.unicast.hosts: ["127.0.0.1:9300","127.0.0.1:9301","127.0.0.1:9302"]
#如果要使用head,那么需要增加新的参数,使head插件可以访问es
http.cors.enabled: true 
http.cors.allow-origin: "*"
```

从节点2：

```yaml
# ================= Elasticsearch Configuration ===================
# 配置es的集群名称, es会自动发现在同一网段下的es,如果在同一网段下有多个集群,就可以用这个属性来区分不同的集群｡ 
cluster.name: elasticsearch
# 节点名称 
node.name: node-003
# 指定该节点是否有资格被选举成为node 
node.master: true 
# 指定该节点是否存储索引数据,默认为true｡ 
cluster.initial_master_nodes: ["127.0.0.1:9300"]
node.data: true 
# 设置绑定的ip地址还有其它节点和该节点交互的ip地址,本机ip 
network.host: 127.0.0.1 
# 指定http端口,你使用head､kopf等相关插件使用的端口
http.port: 9202
# 设置节点间交互的tcp端口,默认是9300｡ 
transport.tcp.port: 9302
#设置集群中master节点的初始列表,可以通过这些节点来自动发现新加入集群的节点｡
#因为下一台elasticsearch的port端口会设置成9301  所以写入两台#elasticsearch地址的完整路径
discovery.zen.ping.unicast.hosts: ["127.0.0.1:9300","127.0.0.1:9301","127.0.0.1:9302"] 
#如果要使用head,那么需要增加新的参数,使head插件可以访问es
http.cors.enabled: true
http.cors.allow-origin: "*" 
```



### 启动集群

先启动主节点，后启动从节点，集群是根据集群的名字

es会自动发现在同一网段下的es节点,如果在同一网段下有多个集群,就可以用这个属性来区分不同的集群

```she
cluster.name: elasticsearch
```

只要配置文件中的集群名相同，自动加入集群

打开es-head,查看集群：

<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1618296550523.png" alt="1618296550523" style="zoom:80%;" />



### 节点故障

#### 从节点故障

<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1618296914379.png" alt="1618296914379" style="zoom:80%;" />



#### 主节点故障

<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1618296956253.png" alt="1618296956253" style="zoom:80%;" />

Master的选举

<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1618297508181.png" alt="1618297508181" style="zoom:80%;" />

根据系统自动生成的版本号决定的，版本号越小，成为主节点越高



##### 脑裂问题

https://www.cnblogs.com/zhukunrong/p/5224558.html

**什么是脑裂？**

简而言之，集群中出现多个主节点



##### **如何避免脑裂问题**

elasticsearch的默认配置很好。但是elasticsearch项目组不可能知道你的特定场景里的所有细节。这就是为什么某些配置参数需要改成适合你的需求的原因。这篇博文里所有提到的参数都可以在你elasticsearch安装地址的**config**目录中的**elasticsearch.yml**中更改。

要预防脑裂问题，我们需要看的一个参数就是 **discovery.zen.minimum_master_nodes**。这个参数决定了在选主过程中需要 有多少个节点通信。缺省配置是1.一个基本的原则是这里需要设置成 N/2+1, N是急群中节点的数量。 例如在一个三节点的集群中， minimum_master_nodes应该被设为 3/2 + 1 = 2(四舍五入)。

让我们想象下之前的情况下如果我们把 **discovery.zen.minimum_master_nodes** 设置成 2（2/2 + 1）。当两个节点的通信失败了，节点1会失去它的主状态，同时节点2也不会被选举为主。没有一个节点会接受索引或搜索的请求，让所有的客户端马上发现这个问题。而且没有一个分片会处于不一致的状态。

我们可以调的另一个参数是 **discovery.zen.ping.timeout**。它的默认值是3秒并且它用来决定一个节点在假设集群中的另一个节点响应失败的情况时等待多久。在一个慢速网络中将这个值调的大一点是个不错的主意。这个参数不止适用于高网络延迟，还能在一个节点超载响应很慢时起作用。



**两节点集群？**

如果你觉得（或直觉上）在一个两节点的集群中把minimum_master_nodes参数设成2是错的，那就对了。在这种情况下如果一个节点挂了，那整个集群就都挂了。尽管这杜绝了脑裂的可能性，但这使elasticsearch另一个好特性 - 用复制分片来构建高可用性 失效了。

如果你刚开始使用elasticsearch，建议配置一个3节点集群。这样你可以设置minimum_master_nodes为2，减少了脑裂的可能性，但仍然保持了高可用的优点：你可以承受一个节点失效但集群还是正常运行的。

但如果已经运行了一个两节点elasticsearch集群怎么办？可以选择为了保持高可用而忍受脑裂的可能性，或者选择为了防止脑裂而选择高可用性。为了避免这种妥协，最好的选择是给集群添加一个节点。这听起来很极端，但并不是。对于每一个elasticsearch节点你可以设置 **node.data** 参数来选择这个节点是否需要保存数据。缺省值是“true”，意思是默认每个elasticsearch节点同时也会作为一个数据节点。

在一个两节点集群，你可以添加一个新节点并把 **node.data** 参数设置为“false”。这样这个节点不会保存任何分片，但它仍然可以被选为主（默认行为）。因为这个节点是一个无数据节点，所以它可以放在一台便宜服务器上。现在你就有了一个三节点的集群，可以安全的把minimum_master_nodes设置为2，避免脑裂而且仍然可以丢失一个节点并且不会丢失数据。

**结论**

脑裂问题很难被彻底解决。在elasticsearch的[问题列表](https://github.com/elasticsearch/elasticsearch/issues/2488)里仍然有关于这个的问题, 描述了在一个极端情况下正确设置了minimum_master_nodes的参数时仍然产生了脑裂问题。 elasticsearch项目组正在致力于开发一个选主算法的更好的实现，但如果你已经在运行elasticsearch集群了那么你需要知道这个潜在的问题。

如何尽快发现这个很重要。一个比较简单的检测问题的方式是，做一个对/_**nodes**下每个节点终端响应的定期检查。这个终端返回一个所有集群节点状态的短报告。如果有两个节点报告了不同的集群列表，那么这是一个产生脑裂状况的明显标志。



为了保守起见，一般建议将**discovery.zen.minimum_master_nodes**大于2分之1改为大于4分之3



#### 集群状态更新

<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1618299696258.png" alt="1618299696258" style="zoom:80%;" />



## 分片

https://blog.csdn.net/qq_38486203/article/details/80077844

https://www.elastic.co/guide/cn/elasticsearch/guide/current/inside-a-shard.html

https://www.jianshu.com/p/cc06f9adbe82



<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1618307352281.png" alt="1618307352281" style="zoom:80%;" />



创建索引`users`, 5个分片，1个副本



 分片可以是**主分片(primary shard)**或者是**复制分片(replica shard)** 

 复制分片只是主分片的一个副本，它可以**防止硬件故障导致的数据丢失，同时可以提供读请求，比如搜索或者从别的 shard 取回文档**。 

这时，我们 把`node-002` 停止掉，集群的数据还是完整的：

<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1618307751380.png" alt="1618307751380" style="zoom:80%;" />

重启启动节点`node-002`, 集群恢复3个节点

<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1618307844886.png" alt="1618307844886" style="zoom: 80%;" />



每个主分片都有一个或多个副本分片，当主分片异常时，副本可以提供数据的查询等操作。主分片和对应的副本分片是不会在同一个节点上的，所以副本分片数的最大值是 n -1（其中 n 为节点数）。

当索引创建完成的时候，主分片的数量就固定了，但是复制分片的数量可以随时调整，根据需求扩大或者缩小规模。如把复制分片的数量从原来的 1 增加到 2 ：

```shell
curl -H "Content-Type: application/json" -XPUT localhost:9200/users/_settings -d '
{
    "number_of_replicas": 2
}'
```



## Net客户端

[net client doc](https://www.elastic.co/guide/en/elasticsearch/client/net-api/current/index.html)

[查询示例](https://www.elastic.co/guide/en/elasticsearch/client/net-api/current/writing-queries.html)

https://study.163.com/course/courseMain.htm?courseId=1210981803



已用包` <PackageReference Include="NEST" Version="7.8.1" />`

**插入数据:**

```C#
var settings = new ConnectionSettings(new Uri(Url.url))
    .DefaultIndex("people");
var client = new ElasticClient(settings);

List<Person> peoples = new List<Person>();
for (int i = 0; i < 10; i++)
{
    Person personinfo = new Person
    {
        Id = i,
        FirstName = "Martijn" + i,
        LastName = "Laarman" + i
    };
    peoples.Add(personinfo);
}

client.IndexMany<Person>(peoples);
```

其中，Url

```C#
public class Url
{
    public static string url = "http://localhost:9200/";
}
```

**查询数据**

```C#
var settings = new ConnectionSettings(new Uri(Url.url))
    .DefaultIndex("people");
var client = new ElasticClient(settings);
var searchResponse = client.Search<Person>(s => s
                                           .From(0)
                                           .Size(10)
                                           .Query(q => q
                                                  .Match(m => m
                                                         .Field(f => f.FirstName)
                                                         .Query("Martijn1")
                                                        )
                                                 )
                                          );
var people = searchResponse.Documents;
Console.WriteLine("查询结果");
foreach (var item in people)
{
    Console.WriteLine($"id:{item.Id},firstname:{item.FirstName},lastname:{item.LastName}");
}
```



集群连接方式

```C#
        var uris = new[]
                {
                      new Uri("http://localhost:9200"),
                      new Uri("http://localhost:9201"),
                      new Uri("http://localhost:9202"),
                };
        var connectionPool = new SniffingConnectionPool(uris);
        var settings = new ConnectionSettings(connectionPool)
            .DefaultIndex("people");
        var client = new ElasticClient(settings);
```





## Logstach

<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1618314821754.png" alt="1618314821754" style="zoom:80%;" />





### Asp.Net Core 对接

### 日志拉取

主要这几个步骤：

1.Asp.Net Core 3.1
2.引用Nlog/Log4
3.配置日志文件保存路径
4.修改logstash配置(文本收集方式)
5.启动logstash服务AAAAA
6.操作程序写入文本日志
7.Kibana查看



新建Asp.Net Core3.1 项目，使用Nlog日志组件，设置Nlog的日志配置文件，将日志输出到文件

```xml
    <target xsi:type="File" name="allfile" fileName="D:/Log/Application/${shortdate}log.log"
            layout="${longdate}|${event-properties:item=EventId_Id}|${uppercase:${level}}|${logger}|${message} ${exception:format=tostring}" />
```

完整的`nlog.config`配置文件如下：

```xml
<?xml version="1.0" encoding="utf-8" ?>
<nlog xmlns="http://www.nlog-project.org/schemas/NLog.xsd"
      xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
      autoReload="true"
      internalLogLevel="Info"
      internalLogFile="c:/temp/${shortdate}.log">

  <!-- enable asp.net core layout renderers -->
  <extensions>
    <add assembly="NLog.Web.AspNetCore"/>
  </extensions>

  <!-- the targets to write to -->
  <targets>
    <!-- write logs to file  -->
    <target xsi:type="File" name="allfile" fileName="D:/Log/Application/${shortdate}log.log"
            layout="${longdate}|${event-properties:item=EventId_Id}|${uppercase:${level}}|${logger}|${message} ${exception:format=tostring}" />

    <!-- another file log, only own logs. Uses some ASP.NET core renderers -->
    <target xsi:type="File" name="ownFile-web" fileName="D:/Log/Application/${shortdate}log.log"
            layout="${longdate}|${event-properties:item=EventId_Id}|${uppercase:${level}}|${logger}|${message} ${exception:format=tostring}|url: ${aspnet-request-url}|action: ${aspnet-mvc-action}" />
  </targets>

  <!-- rules to map from logger name to target -->
  <rules>
    <!--All logs, including from Microsoft-->
    <logger name="*" minlevel="Trace" writeTo="allfile" />

    <!--Skip non-critical Microsoft logs and so log only own logs-->
    <logger name="Microsoft.*" minlevel="Info"  writeTo="blackhole" final="true" />
    <!-- BlackHole without writeTo -->
  </rules>
</nlog>


<!--Trace - very detailed logs，包含大量的信息，例如 protocol payloads。该级别一般仅在开发环境中启用。
Debug - debugging information, 比 Trance 级别稍微粗略，一般在生产环境中不启用。
Info - information messages，一般在生产环境中启用。
Warn - warning messages，一般用于可恢复或临时性错误的非关键问题。
Error - error messages，一般是异常信息。
Fatal - 非常严重的错误！-->
```



新可以建Logstach的配置文件`filelog.conf`，

```json
# Sample Logstash configuration for creating a simple
# Beats -> Logstash -> Elasticsearch pipeline.

input {
 file {
 path => "D:/Log/Application/*log.log"
   start_position => beginning
    }
}
output {
  elasticsearch {
    hosts => ["127.0.0.1:9200"]
    index => "filelog"
    #user => "elastic"
    #password => "changeme"
  }
}

```

> 可以配置有多个文件源
>
> ```json
> # Sample Logstash configuration for creating a simple
> # Beats -> Logstash -> Elasticsearch pipeline.
> 
> input {
>  file {
>  path => "D:/Courseware/ElasticSearch/Zhaoxi.MicroService.LessonService/log/*.txt"
>    start_position => beginning
>     }
>  file {
>  path => "D:/Courseware/ElasticSearch/Zhaoxi.MicroService.ClientDemo/log/*.txt"
>    start_position => beginning
>     }
>  file {
>  path => "D:/Courseware/ElasticSearch/Zhaoxi.MicroService.Service/log/*.txt"
>    start_position => beginning
>     }
> }
> output {
>   elasticsearch {
>     hosts => ["127.0.0.1:9200"]
>     index => "filelog"
>     #user => "elastic"
>     #password => "changeme"
>   }
> }
> 
> ```
>
> 



并把该文件拷贝到`logstash-7.6.1\bin`目录下,打开**CMD**，使用如下命令启动 **Logstash**

```powershell
D:\ProgramFiles\elk\logstash-7.6.1\bin>logstash -f filelog.conf
......
[2021-04-13T20:32:47,397][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
```



打开**Kabana** (http://localhost:5601), 在`Dev Tools`运行如下命令查看现有索引

```pow
GET _cat/indices
```

看到还没有生成索引`filelog`,

现在运行 `Asp.Net Core 3.1`项目，产生日志文件, 其路径为：

```
D:/Log/Application/${shortdate}log.log
```

<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1618318010608.png" alt="1618318010608" style="zoom:80%;" />

```text
2021-04-13 20:46:24.7591||DEBUG|Zhaoxi.ELK.Logstach.Program|init main 
2021-04-13 20:46:27.9744||INFO|Microsoft.Hosting.Lifetime|Application started. Press Ctrl+C to shut down. 
2021-04-13 20:46:27.9805||INFO|Microsoft.Hosting.Lifetime|Hosting environment: Development 
2021-04-13 20:46:28.0097||INFO|Microsoft.Hosting.Lifetime|Content root path: E:\Kzone\CodeLib\ZeroToOne\DotNet\DotNetArchitector\dotnet-architect-01\src\052-053 ELK\src\Zhaoxi.ELK.Logstach_File 
2021-04-13 20:46:28.3401||INFO|Zhaoxi.ELK.Logstach.Controllers.HomeController|{"Code":"6f125c26-0305-4b0b-870e-39f35f6cb669","Message":"这是你们的错误  我没有问题哦"} 

```



这时查看索引，新增了一个`filelog`的索引

```sh
GET _cat/indices

green open filelog                  U_itWszUS42q_on-YagUEQ 1 1 5 0  63.5kb 31.6kb
```

查看日志内容：

```shell
GET /filelog/_search
```

结果：

```json
{
  "took" : 14,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 5,
      "relation" : "eq"
    },
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "filelog",
        "_type" : "_doc",
        "_id" : "rPlEy3gBZLw71mcS6KH5",
        "_score" : 1.0,
        "_source" : {
          "@timestamp" : "2021-04-13T12:46:33.551Z",
          "@version" : "1",
          "message" : """2021-04-13 20:46:24.7591||DEBUG|Zhaoxi.ELK.Logstach.Program|init main 
""",
          "path" : "D:/Log/Application/2021-04-13log.log",
          "host" : "DESKTOP-E6NOF0N"
        }
      },
      {
        "_index" : "filelog",
        "_type" : "_doc",
        "_id" : "rflEy3gBZLw71mcS6qGc",
        "_score" : 1.0,
        "_source" : {
          "@timestamp" : "2021-04-13T12:46:33.595Z",
          "@version" : "1",
          "message" : """2021-04-13 20:46:27.9744||INFO|Microsoft.Hosting.Lifetime|Application started. Press Ctrl+C to shut down. 
""",
          "path" : "D:/Log/Application/2021-04-13log.log",
          "host" : "DESKTOP-E6NOF0N"
        }
      },
      {
        "_index" : "filelog",
        "_type" : "_doc",
        "_id" : "qvlEy3gBZLw71mcS6KH1",
        "_score" : 1.0,
        "_source" : {
          "@timestamp" : "2021-04-13T12:46:33.597Z",
          "@version" : "1",
          "message" : """2021-04-13 20:46:28.0097||INFO|Microsoft.Hosting.Lifetime|Content root path: E:\Kzone\CodeLib\ZeroToOne\DotNet\DotNetArchitector\dotnet-architect-01\src\052-053 ELK\src\Zhaoxi.ELK.Logstach_File 
""",
          "path" : "D:/Log/Application/2021-04-13log.log",
          "host" : "DESKTOP-E6NOF0N"
        }
      },
      {
        "_index" : "filelog",
        "_type" : "_doc",
        "_id" : "qflEy3gBZLw71mcS6KGb",
        "_score" : 1.0,
        "_source" : {
          "@timestamp" : "2021-04-13T12:46:33.597Z",
          "@version" : "1",
          "message" : """2021-04-13 20:46:27.9805||INFO|Microsoft.Hosting.Lifetime|Hosting environment: Development 
""",
          "path" : "D:/Log/Application/2021-04-13log.log",
          "host" : "DESKTOP-E6NOF0N"
        }
      },
      {
        "_index" : "filelog",
        "_type" : "_doc",
        "_id" : "q_lEy3gBZLw71mcS6KH4",
        "_score" : 1.0,
        "_source" : {
          "@timestamp" : "2021-04-13T12:46:33.598Z",
          "@version" : "1",
          "message" : """2021-04-13 20:46:28.3401||INFO|Zhaoxi.ELK.Logstach.Controllers.HomeController|{"Code":"6f125c26-0305-4b0b-870e-39f35f6cb669","Message":"这是你们的错误  我没有问题哦"} 
""",
          "path" : "D:/Log/Application/2021-04-13log.log",
          "host" : "DESKTOP-E6NOF0N"
        }
      }
    ]
  }
}

```



在 **Kibana** 打开`management->Kibana-> Index Patterns(索引模式)`

<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1618318636989.png" alt="1618318636989" style="zoom:80%;" />

点击【创建索引模式】，输入索引模式`filelog` 

<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1618318765804.png" alt="1618318765804" style="zoom:80%;" />

> 可以使用`filelog*`来匹配更多的索引，实现跨索引查询，所以实际工作中，同一类的索引最好统一命名规则



选择筛选字段：

<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1618319213328.png" alt="1618319213328" style="zoom:80%;" />

点击【创建索引模式】：

<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1618319332440.png" alt="1618319332440" style="zoom:80%;" />



打开【Discover】页面，选择【索引模式】，设置查询条件，最后点击【Refresh】

<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1618319655042.png" alt="1618319655042" style="zoom:80%;" />



综上，日志拉取最重要的是一些服务或者系统本地日志

1. 配置数据来源和数据输出
2. 必须给数据出文件的路径，日志的路径和Logstash 得在同一台服务器上
3. 还要维护文件日志，所以不建议使用本地日志, 直接把日志写入ES , [Net Core 使用 Serilog记录日志并输出至ElasticSearch](https://my.oschina.net/u/4287611/blog/3329504)

​                           

### Tcp方式：通过程序推送

新建Asp.Net Core3.1 项目，使用Nlog日志组件，设置Nlog的日志配置文件，将日志输出到 Logstash 提供的接口

```xml
tcp://127.0.0.1:8001
```



完整的`nlog.config`配置文件如下：

```xml
<?xml version="1.0" encoding="utf-8" ?>
<nlog xmlns="http://www.nlog-project.org/schemas/NLog.xsd"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
autoReload="true">
  <!-- the targets to write to -->
  <targets>
    <!--<target xsi:type="Network" name="logError"  address="tcp://127.0.0.1:8001" layout="${longdate} ${uppercase:${level}} ${message}"/>-->

    <target xsi:type="Network" name="logInfo" address="tcp://127.0.0.1:8001" layout="${longdate} ${uppercase:${level}} ${message}"/>

  </targets>
  <!-- rules to map from logger name to target -->
  <rules>
    <!--All logs, including from Microsoft-->
    <!--<logger name="*" minlevel="Trace" writeTo="allfile" />-->
    <!--<logger name="*" minlevel="Error" writeTo="logError" />-->
    <logger name="*" minlevel="Info"  writeTo="logInfo" />
    <logger name="Microsoft.*" maxLevel="Info" final="true" />

  </rules>
</nlog>
```



新建Logstach的配置文件`tcplog.conf`，

```text
input {
 tcp{
   port => 8001
   type => "TcpLog"
 }
}
output {
  elasticsearch {
    hosts => ["127.0.0.1:9200"]
    index => "tcplog"
    #user => "elastic"
    #password => "changeme"
  }
}

```

并把该文件拷贝到`logstash-7.6.1\bin`目录下,打开**CMD**，使用如下命令启动 **Logstash**

```powershell
D:\ProgramFiles\elk\logstash-7.6.1\bin>logstash -f tcplog.conf
......
Successfully started Logstash API endpoint {:port=>9600}
```



打开**Kabana** (http://localhost:5601), 在`Dev Tools`运行如下命令查看现有索引

```pow
GET _cat/indices
```

看到还没有生成索引`tcplog`,

现在运行 `Asp.Net Core 3.1`项目，产生日志文件

```shell
GET _cat/indices
green open tcplog                   sdm0RMIpT3mRt4YITlM7Xw 1 1 11 0    98kb 54.4kb
```



创建索引模式`tcplog*`, 查看日志：

<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1618322274379.png" alt="1618322274379" style="zoom:80%;" />

当然，也可使用Dev Tools 查询：

```shell
GET /tcplog/_search
{
  "query": {"match_phrase": {
    "message": "tcp方式"
  }}
}

```

返回数据：

```json
{
        "_index" : "tcplog",
        "_type" : "_doc",
        "_id" : "t_mDy3gBZLw71mcSAaEf",
        "_score" : 4.371958,
        "_source" : {
          "message" : """2021-04-13 21:54:19.5004 INFO 2222这是文19:27件日志2021-04-13 21:54:22.0738 INFO Application started. Press Ctrl+C to shut down.2021-04-13 21:54:22.0738 INFO Hosting environment: Development2021-04-13 21:54:22.0738 INFO Content root path: E:\Kzone\CodeLib\ZeroToOne\DotNet\DotNetArchitector\dotnet-architect-01\src\052-053 ELK\src\Zhaoxi.ELK.Logstach_TCP2021-04-13 21:54:22.2093 INFO {'a':'aa','b':'bb'}2021-04-13 21:54:22.2093 INFO 20:13***这是tcp方式的日志啊IActionInvoker
""",
          "port" : 54257,
          "host" : "kubernetes.docker.internal",
          "@version" : "1",
          "type" : "TcpLog",
          "@timestamp" : "2021-04-13T13:54:22.237Z"
        }
      }
```



Tcp方式:

Tcp方式方式比日志拉取还要少，基本不用。

因为这种方式主动推送消息给Logstash，属于强推，一旦出现高并发，超出了 Logstash 的处理能力，会引起崩溃。



### Redis 方式

官方文档：https://www.elastic.co/guide/en/logstash/current/plugins-outputs-redis.html

新建.Net 控制台程序：

```C#
static void Main(string[] args)
{
    string listkey = "listlog";
    while (1 == 1)
    {
        Console.WriteLine("请输入发送的内容");
        var message = Console.ReadLine();
        
        using (RedisClient client = new RedisClient("127.0.0.1", 6379, "123456"))
        //using (RedisClient client = new RedisClient("127.0.0.1"))
        {
            client.AddItemToList(listkey, message);
        }
    }
}
```

> 使用redis的八大数据类型中的**list类型**来存储日志



启动控制台程序，将日志写入Redis，如下图所示，插入几条日志：

<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1618330416328.png" alt="1618330416328" style="zoom:80%;" />



新建**Logstach**的配置文件`redislog.conf`，

```text
input {
	redis {
		codec => plain
		host => "127.0.0.1"
		port => 6379
        password => "123456"
		data_type => list
		key => "listlog"
		db => 0
	}
}
output {
  elasticsearch {
    hosts => ["127.0.0.1:9200"]
    index => "redislog"
    #user => "elastic"
    #password => "changeme"
  }
}
```

并把该文件拷贝到`logstash-7.6.1\bin`目录下,打开**CMD**，使用如下命令启动 **Logstash**

```powershell
D:\ProgramFiles\elk\logstash-7.6.1\bin>logstash -f redislog.conf
......
Successfully started Logstash API endpoint {:port=>9600}
```



打开**Kabana** (http://localhost:5601), 在`Dev Tools`运行如下命令查看现有索引

```shell
GET _cat/indices
green open redislog                 3dQuutLPRQe2bVL9BpMlxg 1 1  3 0  29.9kb 14.9kb
```



创建索引模式`redislog*`, 查看日志：

<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1618330966814.png" alt="1618330966814" style="zoom:80%;" />



特别注意：

**Logstash 从 Redis 日志后，会自动把Redis上的键值对删除**



拓展

Logstash 可以把 设置从 Redis 读，然后再写入 Redis 。

创建配置文件`redis1.conf`

```yaml
input {
	redis {
		codec => plain
		host => "127.0.0.1"
		port => 6379
		password => "123456"
		data_type => list
		key => "listlog"
		db => 0
	}
}
output{
	redis{
		codec => plain
		host => ["127.0.0.1:6379"]
		password => "123456"
		data_type => list
		key => logstash
        db => 1
	}
}
```

并把该文件拷贝到`logstash-7.6.1\bin`目录下,打开**CMD**，使用如下命令启动 **Logstash**

```powershell
D:\ProgramFiles\elk\logstash-7.6.1\bin>logstash -f redis1.conf
......
Successfully started Logstash API endpoint {:port=>9600}
```



<img src="images/.Net%E6%9E%B6%E6%9E%84%E5%B8%88%E4%B9%8B%E8%B7%AF(%E4%B8%89)/1618332914374.png" alt="1618332914374" style="zoom:80%;" />



### Kafka 方式

官方文档：https://www.elastic.co/guide/en/logstash/current/plugins-outputs-kafka.html

新建.Net 控制台程序,引用包

```C#
    <PackageReference Include="Confluent.Kafka" Version="1.4.3" />
```

main.cs

```C#
	class Program
	{
		static async System.Threading.Tasks.Task Main(string[] args)
		{
			while (1 == 1)
			{
				Console.WriteLine("请输入发送的内容");
				var message = Console.ReadLine();
				string brokerList = "39.96.82.51:9093";
				await ConfulentKafka.Produce(brokerList, "kafkalog", message);
			}
		}
	}
```

ConfulentKafka.cs

```C#
class ConfulentKafka
	{

		/// <summary>
		/// 发送事件
		/// </summary>
		/// <param name="event"></param>
		public static async Task Produce(string brokerlist, string topicname, string content)
		{

			string brokerList = brokerlist;
			string topicName = topicname;

			var config = new ProducerConfig { BootstrapServers = brokerList };

			using (var producer = new ProducerBuilder<string, string>(config).Build())
			{
				Console.WriteLine("\n-----------------------------------------------------------------------");
				Console.WriteLine($"Producer {producer.Name} producing on topic {topicName}.");
				Console.WriteLine("-----------------------------------------------------------------------");
				try
				{
					var deliveryReport = await producer.ProduceAsync(
						topicName, new Message<string, string> { Key = content, Value = content });

					Console.WriteLine($"delivered to: {deliveryReport.TopicPartitionOffset}");
				}
				catch (ProduceException<string, string> e)
				{
					Console.WriteLine($"failed to deliver message: {e.Message} [{e.Error.Code}]");
				}
			}
		}
	}
```



新建配置文件`kafkalog.conf`

```yaml
input {
     kafka {
      topics => "kafkalog"
      bootstrap_servers => "127.0.0.1:9093"  # 从kafka的leader主机上提取缓存
      codec => "json"  # 在提取kafka主机的日志时，需要写成json格式
            }
}
output {
  elasticsearch {
    hosts => ["127.0.0.1:9200"]
    index => "kafkalog"
    #user => "elastic"
    #password => "changeme"
  }
}

```

并把该文件拷贝到`logstash-7.6.1\bin`目录下,打开**CMD**，使用如下命令启动 **Logstash**

```powershell
D:\ProgramFiles\elk\logstash-7.6.1\bin>logstash -f kafkalog.conf
......
Successfully started Logstash API endpoint {:port=>9600}
```



### 小结

​         推荐使用Redis或Kafka方式，因为这两者能解决高并发的问题。

​        不推荐使用TCP方式，因为高并发会引发问题



# 全链路

最重要的就是数据来源

比如：一次请求，做了10次不同的调用，要使得这10次调用拥有一个唯一的主键，根据时间排序

现在的问题是，怎么用快捷的方式和少写代码的方式让这个10次调用拥有一个唯一的主键

```C#
        static void Main(string[] args)
        {
            //-----------一个业务请求流程：订单
            //生成唯一一个请求Id
            AllLinkModel.RequestID = Guid.NewGuid().ToString();

            new MysqlHelper().Do();
            new RedisHelper().Do();
            new KafkaHelper().Do();

        }
```

每个调用过程，使用相同的`RequestID`去记录日志，

```C#
    public class AllLinkModel
    {
        public static string RequestID { get; set; }
    }
```

而`RequestID`是一个静态字段，有可能其它请求覆盖，导致不是一个请求的里面的调用混淆。

解决方案是使用[数据槽](https://docs.microsoft.com/zh-cn/dotnet/standard/threading/thread-local-storage-thread-relative-static-fields-and-data-slots), 示例 [CallContext 类](https://docs.microsoft.com/zh-cn/dotnet/api/system.runtime.remoting.messaging.callcontext?view=netframework-4.8)



